{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "\n",
    "| Nome        | nUSP           | \n",
    "| ------------- |:-------------:|\n",
    "| Pedro Raul Taborga da Costa | 4537076 |\n",
    "| Flavio de Figueiroa Teixeira Silva | 11270722|\n",
    "\n",
    "**Conjunto de dados escolhido**:\n",
    "Credit Card Fraud Detection (Disponível no [kaggle](https://www.kaggle.com/mlg-ulb/creditcardfraud))\n",
    "\n",
    "**Conteúdo**:\n",
    "O conjunto de dados contém 284.807 transações que ocorreram em dois dias de setembro de 2013, realizadas por pessoas na Europa. O conjunto é altamente desbalanceado, com apenas 0.172% de transações fraudulentas. \n",
    "\n",
    "**Atributos**:\n",
    "Possui 28 atributos numéricos contínuos, derivados das variáveis originais por meio de análise de componentes principais, um atributo numérico que indica a ordem em que as transações ocorreram no tempo e outro atributo numérico com o valor original da transação, totalizando 30 atributos.\n",
    "\n",
    "<details><summary><strong>Atributos detalhados</strong></summary>\n",
    "It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, … V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise. \n",
    "</details>\n",
    "<hr>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Pré Processamento no conjunto de dados"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dtFolder = \"../../datasets/\"\n",
    "df = pd.read_csv(dtFolder+\"creditcardfraud.zip\")\n",
    "cols = df.columns.tolist()\n",
    "reord = [cols.pop(0),cols.pop(-2),cols.pop(-1)]\n",
    "cols = reord + cols\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRen = df\n",
    "dfRen[\"Class\"].value_counts().plot(kind=\"bar\", color={\"green\":\"0\",\"red\":\"1\"}, title=\"Tipo de Transações\") #lol"
   ]
  },
  {
   "source": [
    "## 2. Verificando se existem dados faltantes no conjunto de dados escolhido"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtdNulls = df.isnull().sum(axis = 0).rename(\"Qtd. de null\")\n",
    "qtdNulls"
   ]
  },
  {
   "source": [
    "## Após rebalanceamento\n",
    "Dentre as técnicas de rebalanceamento:\n",
    "\n",
    "1. under sampling -> random\n",
    "1. over sampling -> random, SMOTE (synthetic minority oversampling techinique)\n",
    "1. hibrida -> SMOTE -> TOMEK \n",
    "\n",
    "Escolhemos utilizar random oversampling, visto que a perda de dados por undesampling se mostrou grande mais\n",
    "\n",
    "if !funcionar:\n",
    "Escolhemos utilizar random UNDERsampling, visto que utilizamos tanto undersampling como oversampling porém não houve uma diferença significativa no resultado dos algoritmos, só um aumento colossal no tempo de execução deles. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armazenando a quantidade de entradas de cada tipo de transação\n",
    "count_legal, count_fraude = dfRen[\"Class\"].value_counts()\n",
    "\n",
    "# Separando o dataset em 2 dataframes, cada um contendo só um tipo de transação\n",
    "df_legal = dfRen[dfRen[\"Class\"] == 0].reset_index()\n",
    "df_fraude = dfRen[dfRen[\"Class\"] == 1].reset_index()\n",
    "\n",
    "# Escolhe aleatoriamente o mesmo número que temos em fraudes dentre o total de transações legais\n",
    "df_fraude_over = df_fraude.sample(count_legal, replace=True)\n",
    "\n",
    "# Cria e exibe um dataframe a partir da junção dos 2, agora balanceados\n",
    "df_balanced = pd.concat([df_legal,df_fraude_over], axis=0, ignore_index=True).drop(columns=[\"index\"])\n",
    "df_balanced[\"Class\"].value_counts().plot(kind=\"bar\",color={\"green\":\"Legal\",\"red\":\"Fraude\"},title=\"Tipo de Transações\")"
   ]
  },
  {
   "source": [
    "## 4. Aplicar alguma técnica de normalização nos dados."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalização por MinMaxScaler, onde os valores ficarão entre 0 e 1, que será útil para algoritmos de aprendizado\n",
    "from sklearn import preprocessing\n",
    "mm_scaler = preprocessing.MinMaxScaler()\n",
    "df_mm = mm_scaler.fit_transform(df_balanced)\n",
    "df_norm = pd.DataFrame(df_mm, columns=cols)\n",
    "df_norm"
   ]
  },
  {
   "source": [
    "## 5. Calculando médias, medianas, variâncias, min, max"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = df_norm.describe()\n",
    "desc.drop(index=[\"count\"], inplace=True)\n",
    "desc.loc[\"median\"] = df_norm.median(axis=0)\n",
    "desc.reindex([\"mean\",\"median\",\"std\",\"min\",\"max\",\"25%\",\"50%\",\"75%\"])"
   ]
  },
  {
   "source": [
    "## Matriz de correlação"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "corr_matrix = df_norm.corr()\n",
    "sns.heatmap(corr_matrix, cmap=\"YlGnBu\")"
   ]
  },
  {
   "source": [
    "## Gráficos para analisar os dados"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ????"
   ]
  },
  {
   "source": [
    "## Exportando o conjunto de dados rebalanceado e normalizado"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: blabla"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}